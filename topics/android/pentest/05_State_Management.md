# State Management: Memory Beyond Tokens

## 1. Overview: The Context Paradox

A typical Pentest generates 500k+ tokens of logs, code, and thoughts.
Modern LLMs have 128k - 200k context windows.
**Problem**: We cannot stuff the whole specific pentest into the general brain. behaviors degrade, and costs explode.

**Solution**: **External State Management**. The AI is "Stateless" between turns. The **Database** holds the Truth.

---

## 2. The "Case File" Database

We use a local `SQLite` database (`case_data.db`) for each engagement.

### 2.1 Schema Design

#### Table: `findings` (The Vulnerability Ledger)
| Column          | Type | Description                                            |
| :-------------- | :--- | :----------------------------------------------------- |
| `id`            | UUID | Unique ID of the finding.                              |
| `category`      | TEXT | OWASP Category (e.g., M1: Improper Platform Usage).    |
| `status`        | ENUM | `POTENTIAL`, `VERIFIED`, `FALSE_POSITIVE`, `FIXED`.    |
| `evidence_path` | PATH | Path to the specific Logcat/Screenshot artifact.       |
| `human_notes`   | TEXT | **The Source of Truth**. What the human said about it. |

#### Table: `tasks` (The Operational State)
| Column             | Type     | Description                                      |
| :----------------- | :------- | :----------------------------------------------- |
| `id`               | INT      | Sequence number.                                 |
| `agent_role`       | ENUM     | `TRIAGE`, `FUZZER`, `SCRIBE`.                    |
| `command_executed` | TEXT     | The exact SCP command (e.g., `/fuzz ...`).       |
| `result_summary`   | TEXT     | Short summary: "Crashed app after 500 attempts". |
| `timestamp`        | DATETIME | Execution time.                                  |

#### Table: `knowledge_graph` (The App Model)
| Column        | Type | Description                                           |
| :------------ | :--- | :---------------------------------------------------- |
| `node`        | TEXT | `LoginActivity`, `api/v1/user`, `db_credentials`.     |
| `type`        | ENUM | `ACTIVITY`, `API_ENDPOINT`, `SECRET`.                 |
| `connections` | JSON | `["exported_by:Manifest", "called_by:MainActivity"]`. |

---

## 3. Context Slicing Strategy

How do we create the "Prompt" for the AI agent? We **Slice** the database.

### 3.1 The "Focus Mode"
When the Human says: *"Analyze LoginActivity"*, the System queries the DB:
1.  **Select Code**: `Decompiler` -> `LoginActivity.smali` (limit 2000 lines).
2.  **Select Related**: `knowledge_graph` where `connection` includes `LoginActivity`.
3.  **Select History**: `tasks` where `command` involved `LoginActivity`.

**Prompt Construction**:
```text
SYSTEM: You are the Triage Agent.
CONTEXT:
1. Target: LoginActivity (Code attached).
2. Related: Starts 'HomeActivity' on success.
3. History: Fuzzer tried 'admin/admin' yesterday and failed.
TASK: Develop a new bypass strategy excluding 'admin/admin'.
```
*Result*: Total Context = 5k tokens. Highly Focused. No Hallucination about unrelated files.

---

## 4. Session Preservation & Resumption

### 4.1 The "Brain Dump"
Every 10 minutes, or after every SCP Command completion:
1.  System forces AI to summarize its "Short Term Memory".
2.  System saves summary to `tasks` table.
3.  System flushes LLM Context.

### 4.2 The "Cold Start"
When the Engineer opens the project next week:
1.  System loads `findings` (Where were we?).
2.  System loads top 5 recent `tasks` (What did we just do?).
3.  System prompts AI: *"We are resuming. Last status: Verified SQLi. Next step: Assessing Impact."*

**Benefit**:
*   **Cost**: Zero token cost for idle time.
*   **Reliability**: No "Drift" or degradation over long engagements.
*   **Collaboration**: Another engineer can open the DB and see the exact state without needing the chat history.
